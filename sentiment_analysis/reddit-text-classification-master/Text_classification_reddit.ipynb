{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EniPFeS_4bAv"
      },
      "source": [
        "# RUN THIS: IMPORT EVERYTHING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SkXtI6wL3rim"
      },
      "outputs": [],
      "source": [
        "#ALL Imports\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy import sparse\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vix3jO6rEI57"
      },
      "source": [
        "## Run This: SpaCy and Tokenize Normalize Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "CFtq-JxSjoRb",
        "outputId": "baa4d4ac-8269-4b00-a63a-5d56ae39e22f"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "[E955] Can't find table(s) lemma_rules for language 'en' in spacy-lookups-data. Make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Make sure to set lemmatizer to look for POS (Part-of-Speech)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemmatizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GIT/NLPStudy/nlpenv/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:159\u001b[0m, in \u001b[0;36mLemmatizer.initialize\u001b[0;34m(self, get_examples, nlp, lookups)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLemmatizer: loading tables from spacy-lookups-data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m     lookups \u001b[38;5;241m=\u001b[39m \u001b[43mload_lookups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired_tables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     optional_lookups \u001b[38;5;241m=\u001b[39m load_lookups(\n\u001b[1;32m    161\u001b[0m         lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mlang, tables\u001b[38;5;241m=\u001b[39moptional_tables, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m optional_lookups\u001b[38;5;241m.\u001b[39mtables:\n",
            "File \u001b[0;32m~/Documents/GIT/NLPStudy/nlpenv/lib/python3.12/site-packages/spacy/lookups.py:30\u001b[0m, in \u001b[0;36mload_lookups\u001b[0;34m(lang, tables, strict)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m registry\u001b[38;5;241m.\u001b[39mlookups:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tables) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE955\u001b[38;5;241m.\u001b[39mformat(table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tables), lang\u001b[38;5;241m=\u001b[39mlang))\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lookups\n\u001b[1;32m     32\u001b[0m data \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mlookups\u001b[38;5;241m.\u001b[39mget(lang)\n",
            "\u001b[0;31mValueError\u001b[0m: [E955] Can't find table(s) lemma_rules for language 'en' in spacy-lookups-data. Make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language."
          ]
        }
      ],
      "source": [
        "#!python -m spacy download en\n",
        "\n",
        "\n",
        "# Load the small english model. \n",
        "# Disable the advanced NLP features in the pipeline for efficiency.\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "\n",
        "from spacy.lookups import load_lookups\n",
        "\n",
        "# Load a blank English pipeline\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add necessary components\n",
        "nlp.add_pipe(\"tagger\")\n",
        "nlp.add_pipe(\"attribute_ruler\")\n",
        "nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"})\n",
        "\n",
        "# Load the lookup tables\n",
        "lookups = load_lookups(\"en\", tables=[\"lemma_rules\", \"lemma_index\", \"lemma_exc\", \"lemma_lookup\"])\n",
        "nlp.get_pipe(\"lemmatizer\").initialize(nlp.vocab, lookups=lookups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "x4O_xWjjEHo7"
      },
      "outputs": [],
      "source": [
        "#@Tokenize\n",
        "def spacy_tokenize(string):\n",
        "  tokens = list()\n",
        "  doc = nlp(string)\n",
        "  for token in doc:\n",
        "    tokens.append(token)\n",
        "  return tokens\n",
        "\n",
        "#@Normalize\n",
        "def normalize(tokens):\n",
        "  normalized_tokens = list()\n",
        "  for token in tokens:\n",
        "    if (token.is_alpha or token.is_digit):\n",
        "      normalized = token.text.lower().strip()\n",
        "      normalized_tokens.append(normalized)\n",
        "  return normalized_tokens\n",
        "\n",
        "#@Tokenize and normalize\n",
        "def tokenize_normalize(string):\n",
        "  return normalize(spacy_tokenize(string))  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Vtm-bgpACY-"
      },
      "source": [
        "## ItemSelector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZYwbGy7O8mHx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dO_bFCr64wWw"
      },
      "source": [
        "## Run this evaluation summary & confusion matrix function & create plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "j-U3Oj5YaxE2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_conf_matrix(test_labels, prediction, classes, title):\n",
        "  conf_arr = confusion_matrix(test_labels, prediction)\n",
        "  # print(conf_arr[0].size)\n",
        "  norm_conf = []\n",
        "  for i in conf_arr:\n",
        "      a = 0\n",
        "      tmp_arr = []\n",
        "      a = sum(i, 0)\n",
        "      for j in i:\n",
        "          tmp_arr.append(float(j)/float(a))\n",
        "      norm_conf.append(tmp_arr)\n",
        "  # print(norm_conf)\n",
        "  fig = plt.figure(figsize=(10, 6))\n",
        "  plt.clf()\n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.set_aspect(1)\n",
        "  res = ax.imshow(np.array(norm_conf), cmap=plt.cm.terrain, \n",
        "                  interpolation='nearest')\n",
        "\n",
        "  width, height = conf_arr.shape\n",
        "\n",
        "  for x in range(width):\n",
        "      for y in range(height):\n",
        "          ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n",
        "                      horizontalalignment='center',\n",
        "                      verticalalignment='center')\n",
        "\n",
        "  cb = fig.colorbar(res)\n",
        "  cb.set_label('$Probability$')\n",
        "  cb.set_ticks(np.linspace(0,1,11))\n",
        "  # cb.set_ticklabels(('0','50','100','150','200','250','300','70','80','90','100'))\n",
        "  classes = classes\n",
        "  plt.xticks(range(width), classes[:width],rotation='vertical')\n",
        "  plt.yticks(range(height), classes[:height])\n",
        "  plt.grid(axis=\"y\")\n",
        "  plt.grid(axis=\"x\")\n",
        "  plt.ylabel='True Label'\n",
        "  plt.xlabel = 'Predicted Label'\n",
        "  plt.title = title\n",
        "  \n",
        "  name = title+'confusion_matrix.png'\n",
        "  plt.savefig(name, format='png')\n",
        "\n",
        "#@title Run this confusion matrix function\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "  \n",
        "  \n",
        "\n",
        "def create_plot(text, model, prediction, test_labels):\n",
        "  precision, reacall, fscore, support = score(prediction,test_labels)\n",
        "\n",
        "  bars = np.arange(len(model.classes_))\n",
        "  classes = model.classes_\n",
        "  width=.4\n",
        "  fig, ax=plt.subplots(1,1,figsize=(8,6))\n",
        "  rects1 = ax.bar(bars, fscore, width)\n",
        "\n",
        "  ax.set_ylabel('F1')\n",
        "  ax.set_title(text)\n",
        "  ax.set_xticks(bars+width/3)\n",
        "  plt.xticks(rotation='vertical')\n",
        "  ax.set_xticklabels(classes)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  \n",
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1,average='macro') #1 means f_1 measure\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(y_true=true_labels, y_pred=predictions))#,labels=true_labels)) # Note the order here is true, predicted, odd.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2gv2i9fNDvrg"
      },
      "source": [
        "# Part A: Subreddit Prediction ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "II4SW70_m0lf"
      },
      "source": [
        "#Q1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yt8ym-rZeA53"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "ZG9BpbQt3-ko",
        "outputId": "70cc79e2-9a0e-4d71-f12c-a0ff42d8e286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying gs://textasdata/coursework/coursework_subreddit_train.json...\n",
            "\\ [1 files][ 10.1 MiB/ 10.1 MiB]                                                \n",
            "Operation completed over 1 objects/10.1 MiB.                                     \n",
            "Copying gs://textasdata/coursework/coursework_subreddit_test.json...\n",
            "\\ [1 files][  2.7 MiB/  2.7 MiB]                                                \n",
            "Operation completed over 1 objects/2.7 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "subreddit_train = \"coursework_subreddit_train.json\"\n",
        "subreddit_test = \"coursework_subreddit_test.json\"\n",
        "\n",
        "!gsutil cp gs://textasdata/coursework/coursework_subreddit_train.json $subreddit_train \n",
        "!gsutil cp gs://textasdata/coursework/coursework_subreddit_test.json  $subreddit_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "colab_type": "code",
        "id": "iCEG8t6PC2f7",
        "outputId": "e2f6e6fa-8f4d-43e1-835c-16a7a69b6a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['is_self_post', 'posts', 'subreddit', 'title', 'url']\n",
            "   is_self_post                                              posts  \\\n",
            "0           1.0  [{'body': 'I think everyone has that one frien...   \n",
            "1           1.0  [{'body': 'I not 100% sure this is the right p...   \n",
            "2           1.0  [{'body': '', 'author': 'Leisure321', 'url': '...   \n",
            "3           1.0  [{'body': 'It's called 'forgetting things'.', ...   \n",
            "4           1.0  [{'body': 'How would I do this? I am looking t...   \n",
            "\n",
            "        subreddit                                              title  \\\n",
            "0   relationships  How do I [23F] communicate with my self-center...   \n",
            "1  summonerschool  What Cherry switch do you recommend for League...   \n",
            "2       askreddit                   Where do memes go when they die?   \n",
            "3           trees                     Some weird long term affects??   \n",
            "4        buildapc  Simple question: If I install Windows to a sta...   \n",
            "\n",
            "                                                 url  \n",
            "0  https://www.reddit.com/r/relationships/comment...  \n",
            "1  https://www.reddit.com/r/summonerschool/commen...  \n",
            "2  https://www.reddit.com/r/AskReddit/comments/4d...  \n",
            "3  https://www.reddit.com/r/trees/comments/1h300m...  \n",
            "4  https://www.reddit.com/r/buildapc/comments/jhb...  \n",
            "7280\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "## threads\n",
        "\n",
        "train_threads = pd.read_json(path_or_buf=subreddit_train, lines=True)\n",
        "print(list(train_threads.columns.values))\n",
        "print(train_threads.head())\n",
        "print(train_threads.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "colab_type": "code",
        "id": "KhdO37KFieGJ",
        "outputId": "4ba42913-37f0-4dd8-cfca-1005e26602e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1456, 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_self_post</th>\n",
              "      <th>posts</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>[{'body': 'I think everyone has that one frien...</td>\n",
              "      <td>relationships</td>\n",
              "      <td>How do I [23F] communicate with my self-center...</td>\n",
              "      <td>https://www.reddit.com/r/relationships/comment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>[{'body': 'I not 100% sure this is the right p...</td>\n",
              "      <td>summonerschool</td>\n",
              "      <td>What Cherry switch do you recommend for League...</td>\n",
              "      <td>https://www.reddit.com/r/summonerschool/commen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>[{'body': '', 'author': 'Leisure321', 'url': '...</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>Where do memes go when they die?</td>\n",
              "      <td>https://www.reddit.com/r/AskReddit/comments/4d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>[{'body': 'It's called 'forgetting things'.', ...</td>\n",
              "      <td>trees</td>\n",
              "      <td>Some weird long term affects??</td>\n",
              "      <td>https://www.reddit.com/r/trees/comments/1h300m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>[{'body': 'How would I do this? I am looking t...</td>\n",
              "      <td>buildapc</td>\n",
              "      <td>Simple question: If I install Windows to a sta...</td>\n",
              "      <td>https://www.reddit.com/r/buildapc/comments/jhb...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_self_post                                              posts  \\\n",
              "0           1.0  [{'body': 'I think everyone has that one frien...   \n",
              "1           1.0  [{'body': 'I not 100% sure this is the right p...   \n",
              "2           1.0  [{'body': '', 'author': 'Leisure321', 'url': '...   \n",
              "3           1.0  [{'body': 'It's called 'forgetting things'.', ...   \n",
              "4           1.0  [{'body': 'How would I do this? I am looking t...   \n",
              "\n",
              "        subreddit                                              title  \\\n",
              "0   relationships  How do I [23F] communicate with my self-center...   \n",
              "1  summonerschool  What Cherry switch do you recommend for League...   \n",
              "2       askreddit                   Where do memes go when they die?   \n",
              "3           trees                     Some weird long term affects??   \n",
              "4        buildapc  Simple question: If I install Windows to a sta...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.reddit.com/r/relationships/comment...  \n",
              "1  https://www.reddit.com/r/summonerschool/commen...  \n",
              "2  https://www.reddit.com/r/AskReddit/comments/4d...  \n",
              "3  https://www.reddit.com/r/trees/comments/1h300m...  \n",
              "4  https://www.reddit.com/r/buildapc/comments/jhb...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(train_threads.shape)\n",
        "train_threads.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "colab_type": "code",
        "id": "89UU3g27C8SZ",
        "outputId": "056ebf78-540e-4cda-fd7c-5029bb07a9a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   is_self_post                                              posts  \\\n",
            "0           1.0  [{'body': 'Was watching a VOD from last years ...   \n",
            "1           1.0  [{'body': 'Basically what the title says.', 'u...   \n",
            "2           1.0  [{'body': '', 'author': 'Daft-Punk', 'url': 'h...   \n",
            "3           1.0  [{'body': 'I start running this year. I do it ...   \n",
            "4           1.0  [{'body': '[deleted]', 'url': 'https://www.red...   \n",
            "\n",
            "       subreddit                                              title  \\\n",
            "0      starcraft  Just a reminder on how much SC2 has evolved th...   \n",
            "1    whowouldwin  Your Favorite Hero Now Has A Healing Factor As...   \n",
            "2      askreddit  If you could live anywhere in the world, where...   \n",
            "3      askreddit                   Do you ever get use to exercise?   \n",
            "4  tipofmytongue         [TOMT] [book] A scary french book for kids   \n",
            "\n",
            "                                                 url  \n",
            "0  https://www.reddit.com/r/starcraft/comments/mq...  \n",
            "1  https://www.reddit.com/r/whowouldwin/comments/...  \n",
            "2  https://www.reddit.com/r/AskReddit/comments/27...  \n",
            "3  https://www.reddit.com/r/AskReddit/comments/x9...  \n",
            "4  https://www.reddit.com/r/tipofmytongue/comment...  \n",
            "1825\n"
          ]
        }
      ],
      "source": [
        "test_threads = pd.read_json(path_or_buf=subreddit_test, lines=True)\n",
        "print(test_threads.head())\n",
        "print(test_threads.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "colab_type": "code",
        "id": "3Nl9qzazDQ_6",
        "outputId": "ce1c8ae1-cea4-4295-e728-ad199a8ea41d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count     20.000000\n",
            "mean      72.800000\n",
            "std       73.368285\n",
            "min       28.000000\n",
            "25%       36.250000\n",
            "50%       45.500000\n",
            "75%       63.750000\n",
            "max      334.000000\n",
            "Name: count, dtype: float64\n",
            "subreddit\n",
            "askreddit               334\n",
            "leagueoflegends         196\n",
            "buildapc                131\n",
            "explainlikeimfive        82\n",
            "trees                    66\n",
            "techsupport              63\n",
            "pcmasterrace             62\n",
            "gaming                   62\n",
            "electronic_cigarette     59\n",
            "relationships            48\n",
            "tipofmytongue            43\n",
            "hearthstone              38\n",
            "jailbreak                38\n",
            "summonerschool           37\n",
            "atheism                  37\n",
            "reddit.com               34\n",
            "movies                   33\n",
            "whowouldwin              33\n",
            "personalfinance          32\n",
            "starcraft                28\n",
            "Name: count, dtype: int64\n",
            "['askreddit', 'leagueoflegends', 'buildapc', 'explainlikeimfive', 'trees', 'techsupport', 'pcmasterrace', 'gaming', 'electronic_cigarette', 'relationships', 'tipofmytongue', 'hearthstone', 'jailbreak', 'summonerschool', 'atheism', 'reddit.com', 'movies', 'whowouldwin', 'personalfinance', 'starcraft']\n"
          ]
        }
      ],
      "source": [
        "subreddit_counts = train_threads['subreddit'].value_counts()\n",
        "print(subreddit_counts.describe())\n",
        "top_subbreddits = subreddit_counts.nlargest(20)\n",
        "top_subbreddits_list = top_subbreddits.index.tolist()\n",
        "print(top_subbreddits)\n",
        "print(top_subbreddits_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d4E5w3GueG6s"
      },
      "source": [
        "## Train & test data frame creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "g-osdXAtioE2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_threads_frame(subreddit_file):\n",
        "  \n",
        "  posts = list()\n",
        "\n",
        "  # If the dataset is too large, you can load a subset of the posts.\n",
        "  post_limit = 100000000\n",
        "\n",
        "  # Construct a dataframe, by opening the JSON file line-by-line\n",
        "  with open(subreddit_file) as jsonfile:\n",
        "    for i, line in enumerate(jsonfile):\n",
        "      thread = json.loads(line)\n",
        "      if (len(posts) > post_limit):\n",
        "        break\n",
        "      sub=''\n",
        "      title=''\n",
        "      url=''\n",
        "      id_=''\n",
        "      author=''\n",
        "      body=''\n",
        "      \n",
        "      for post in thread['posts']:\n",
        "        sub =thread['subreddit']\n",
        "        title = thread['title']\n",
        "        url = thread['url']\n",
        "        id_ += \" | \" +post.get('id',\"\")\n",
        "        author+= \" | \" + post.get('author', \"\")\n",
        "        body+=\" | \" +post.get('body', \"\")\n",
        "        \n",
        "      posts.append((sub, title, url, id_, author, body))\n",
        "\n",
        "\n",
        "  labels = ['subreddit', 'title', 'url', 'id', 'author', 'body']\n",
        "  return(pd.DataFrame(posts, columns=labels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "72w6OGJ8aUBp"
      },
      "outputs": [],
      "source": [
        "train_threads_frame = create_threads_frame(subreddit_train)\n",
        "test_threads_frame = create_threads_frame(subreddit_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "colab_type": "code",
        "id": "3T84jg02VjGa",
        "outputId": "c751a7a5-302c-4fd3-8b62-00dd4576b9c0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>relationships</td>\n",
              "      <td>How do I [23F] communicate with my self-center...</td>\n",
              "      <td>https://www.reddit.com/r/relationships/comment...</td>\n",
              "      <td>| t1_covzqua | t1_cow04yo | t1_cow4211 | t1_c...</td>\n",
              "      <td>| Pouritdownmythroat | WhyFrankWhy | Pouritdo...</td>\n",
              "      <td>| I think everyone has that one friend who lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>summonerschool</td>\n",
              "      <td>What Cherry switch do you recommend for League...</td>\n",
              "      <td>https://www.reddit.com/r/summonerschool/commen...</td>\n",
              "      <td>| t3_2w8jon | t1_cooiwv5 | t1_coojoh9 | t1_co...</td>\n",
              "      <td>|  |  | ThisGermanGuy | shaunrnm | Sub_Salac ...</td>\n",
              "      <td>| I not 100% sure this is the right place to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Where do memes go when they die?</td>\n",
              "      <td>https://www.reddit.com/r/AskReddit/comments/4d...</td>\n",
              "      <td>| t3_4dr951 | t1_d1tjpa2 | t1_d1tjolo | t1_d1...</td>\n",
              "      <td>| Leisure321 | Zeolance |  | Buttersgoo23 | q...</td>\n",
              "      <td>|  | Facebook |  | 9gag | Memes never truly d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>trees</td>\n",
              "      <td>Some weird long term affects??</td>\n",
              "      <td>https://www.reddit.com/r/trees/comments/1h300m...</td>\n",
              "      <td>| t1_caqbp3q | t1_caqbzo7 | t1_caqcfm5</td>\n",
              "      <td>| refugee4chan | donquixote6179 | ThatStonedA...</td>\n",
              "      <td>| It's called 'forgetting things'. | Dude i d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>Simple question: If I install Windows to a sta...</td>\n",
              "      <td>https://www.reddit.com/r/buildapc/comments/jhb...</td>\n",
              "      <td>| t3_jhbqc | t1_c2c566a | t1_c2c3s12 | t1_c2c...</td>\n",
              "      <td>| catalyzeme | uses | rvabdn | greg2709 | spy...</td>\n",
              "      <td>| How would I do this? I am looking to wait a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        subreddit                                              title  \\\n",
              "0   relationships  How do I [23F] communicate with my self-center...   \n",
              "1  summonerschool  What Cherry switch do you recommend for League...   \n",
              "2       askreddit                   Where do memes go when they die?   \n",
              "3           trees                     Some weird long term affects??   \n",
              "4        buildapc  Simple question: If I install Windows to a sta...   \n",
              "\n",
              "                                                 url  \\\n",
              "0  https://www.reddit.com/r/relationships/comment...   \n",
              "1  https://www.reddit.com/r/summonerschool/commen...   \n",
              "2  https://www.reddit.com/r/AskReddit/comments/4d...   \n",
              "3  https://www.reddit.com/r/trees/comments/1h300m...   \n",
              "4  https://www.reddit.com/r/buildapc/comments/jhb...   \n",
              "\n",
              "                                                  id  \\\n",
              "0   | t1_covzqua | t1_cow04yo | t1_cow4211 | t1_c...   \n",
              "1   | t3_2w8jon | t1_cooiwv5 | t1_coojoh9 | t1_co...   \n",
              "2   | t3_4dr951 | t1_d1tjpa2 | t1_d1tjolo | t1_d1...   \n",
              "3             | t1_caqbp3q | t1_caqbzo7 | t1_caqcfm5   \n",
              "4   | t3_jhbqc | t1_c2c566a | t1_c2c3s12 | t1_c2c...   \n",
              "\n",
              "                                              author  \\\n",
              "0   | Pouritdownmythroat | WhyFrankWhy | Pouritdo...   \n",
              "1   |  |  | ThisGermanGuy | shaunrnm | Sub_Salac ...   \n",
              "2   | Leisure321 | Zeolance |  | Buttersgoo23 | q...   \n",
              "3   | refugee4chan | donquixote6179 | ThatStonedA...   \n",
              "4   | catalyzeme | uses | rvabdn | greg2709 | spy...   \n",
              "\n",
              "                                                body  \n",
              "0   | I think everyone has that one friend who lo...  \n",
              "1   | I not 100% sure this is the right place to ...  \n",
              "2   |  | Facebook |  | 9gag | Memes never truly d...  \n",
              "3   | It's called 'forgetting things'. | Dude i d...  \n",
              "4   | How would I do this? I am looking to wait a...  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_threads_frame.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r5_NfLTHkJNq"
      },
      "source": [
        "## Set train and test labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5dqKpBxxkIZR"
      },
      "outputs": [],
      "source": [
        "train_labels = train_threads_frame['subreddit']\n",
        "test_labels = test_threads_frame['subreddit']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9laVDp8XHer8"
      },
      "outputs": [],
      "source": [
        "sub_classes=[ 'askreddit','atheism','buildapc','electronic_cigarette','explainlikeimfive', 'gaming',     \n",
        "         'hearthstone','jailbreak','leagueoflegends','movies','pcmasterrace','personalfinance','reddit.com','relationships','starcraft','summonerschool','techsupport',     \n",
        "       'tipofmytongue',  'trees', 'whowouldwin' ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ElHlZIOVjl1z"
      },
      "source": [
        "## One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LAK5qdUhOL_o"
      },
      "source": [
        "### Hstack\n",
        "\n",
        "Vectorize features independently and combine them using an HStack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uDZP3jbDipOc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/chancemaker/Documents/GIT/NLPStudy/nlpenv/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy import sparse\n",
        "\n",
        "# Preprocess your text (tokenization and normalization) manually\n",
        "def preprocess_data(data):\n",
        "    return [' '.join(tokenize_normalize(text)) for text in data]\n",
        "\n",
        "# Preprocess each column separately\n",
        "train_body_preprocessed = preprocess_data(train_threads_frame['body'])\n",
        "test_body_preprocessed = preprocess_data(test_threads_frame['body'])\n",
        "\n",
        "train_author_preprocessed = preprocess_data(train_threads_frame['author'])\n",
        "test_author_preprocessed = preprocess_data(test_threads_frame['author'])\n",
        "\n",
        "train_title_preprocessed = preprocess_data(train_threads_frame['title'])\n",
        "test_title_preprocessed = preprocess_data(test_threads_frame['title'])\n",
        "\n",
        "# Initialize CountVectorizer (without custom tokenizer)\n",
        "one_hot_vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "# Fit and transform preprocessed text\n",
        "train_features_onehot_body = one_hot_vectorizer.fit_transform(train_body_preprocessed)\n",
        "test_features_onehot_body = one_hot_vectorizer.transform(test_body_preprocessed)\n",
        "\n",
        "train_features_onehot_auth = one_hot_vectorizer.fit_transform(train_author_preprocessed)\n",
        "test_features_onehot_auth = one_hot_vectorizer.transform(test_author_preprocessed)\n",
        "\n",
        "train_features_onehot_title = one_hot_vectorizer.fit_transform(train_title_preprocessed)\n",
        "test_features_onehot_title = one_hot_vectorizer.transform(test_title_preprocessed)\n",
        "\n",
        "# Combine features\n",
        "train_features_onehot = sparse.hstack([train_features_onehot_body, train_features_onehot_auth, train_features_onehot_title])\n",
        "test_features_onehot = sparse.hstack([test_features_onehot_body, test_features_onehot_auth, test_features_onehot_title])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JKWj3e9q-SEx"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1074
        },
        "colab_type": "code",
        "id": "V4TcJvw-NkGl",
        "outputId": "919539c1-24f8-443c-f11c-b36ceb2bff6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation for: LR One-Hot Encoding\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/chancemaker/Documents/GIT/NLPStudy/nlpenv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/chancemaker/Documents/GIT/NLPStudy/nlpenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "too many positional arguments",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mfit(train_features_onehot,train_labels)\n\u001b[1;32m      4\u001b[0m lr_predict1 \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mpredict(test_features_onehot)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mevaluation_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLR One-Hot Encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_predict1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m                   \n",
            "Cell \u001b[0;32mIn[6], line 124\u001b[0m, in \u001b[0;36mevaluation_summary\u001b[0;34m(description, predictions, true_labels)\u001b[0m\n\u001b[1;32m    122\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(predictions, true_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    123\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(predictions, true_labels)\n\u001b[0;32m--> 124\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#1 means f_1 measure\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has Acc=\u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124m P=\u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124m R=\u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124m F1=\u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (description,accuracy,precision,recall,f1))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(predictions, true_labels, digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n",
            "File \u001b[0;32m~/Documents/GIT/NLPStudy/nlpenv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_sig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.12/inspect.py:3267\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3264\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.12/inspect.py:3191\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   3189\u001b[0m         \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[1;32m   3190\u001b[0m         \u001b[38;5;66;03m# argument\u001b[39;00m\n\u001b[0;32m-> 3191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3192\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m _VAR_POSITIONAL:\n\u001b[1;32m   3195\u001b[0m         \u001b[38;5;66;03m# We have an '*args'-like argument, let's fill it with\u001b[39;00m\n\u001b[1;32m   3196\u001b[0m         \u001b[38;5;66;03m# all positional arguments we have left and move on to\u001b[39;00m\n\u001b[1;32m   3197\u001b[0m         \u001b[38;5;66;03m# the next phase\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m         values \u001b[38;5;241m=\u001b[39m [arg_val]\n",
            "\u001b[0;31mTypeError\u001b[0m: too many positional arguments"
          ]
        }
      ],
      "source": [
        "\n",
        "## one-hot and logistic regression\n",
        "lr = LogisticRegression(solver='saga')\n",
        "lr_model = lr.fit(train_features_onehot,train_labels)\n",
        "lr_predict1 = lr.predict(test_features_onehot)\n",
        "evaluation_summary(\"LR One-Hot Encoding\", lr_predict1, test_labels)                   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cS2TismDFRc2"
      },
      "source": [
        "#### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "colab_type": "code",
        "id": "PEHn7MNxgT4J",
        "outputId": "fba9febf-a4e1-4646-83c7-4744b36c55fb"
      },
      "outputs": [],
      "source": [
        "create_plot('One-hot-encoding and Logistic Regression F1 scores', lr_model, lr_predict1, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "colab_type": "code",
        "id": "rGcTR8GC7GuV",
        "outputId": "3a2b46bf-8848-4da4-a896-c5b2e9b0e830"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(test_labels, lr_predict1, sub_classes,title='Confusion matrix, without normalization')\n",
        "plot_conf_matrix(test_labels,lr_predict1, sub_classes, title= 'One-hot LR Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2AhQCVPl-VuW"
      },
      "source": [
        "### SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "colab_type": "code",
        "id": "S6IGdqsxOA-h",
        "outputId": "f46cb887-65e8-4009-9901-ca185a141459"
      },
      "outputs": [],
      "source": [
        "\n",
        "svc1 = SVC(gamma='auto',kernel='rbf')\n",
        "svc_model1 = svc1.fit(train_features_onehot,train_labels)\n",
        "svc_predict1= svc1.predict(test_features_onehot)\n",
        "evaluation_summary(\"SVC One-Hot Encoding\", svc_predict1, test_labels) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E6U7DvyTRNHB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SHmn9LmN-aJL"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "colab_type": "code",
        "id": "NuQ37plQOI6s",
        "outputId": "26bd91b2-fdf4-4add-a686-c945dc666626"
      },
      "outputs": [],
      "source": [
        "nb1 = BernoulliNB()\n",
        "nb_model1 = nb1.fit(train_features_onehot, train_labels)\n",
        "nb_predict1 = nb1.predict(test_features_onehot)\n",
        "evaluation_summary(\"NB One-Hot Encoding\", nb_predict1, test_labels) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dp4AMA1Swl3m"
      },
      "source": [
        "### Dummy Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1023
        },
        "colab_type": "code",
        "id": "h4S6r9a-OXZJ",
        "outputId": "a21430e7-6319-443f-c03a-9b259e4e0d82"
      },
      "outputs": [],
      "source": [
        "\n",
        "dummy_mf1 = DummyClassifier(strategy='most_frequent')\n",
        "dummy_mf1.fit(train_features_onehot, train_labels)\n",
        "print(dummy_mf1.score(test_features_onehot, test_labels))\n",
        "evaluation_summary(\"Dummy Majority\", dummy_mf1.predict(test_features_onehot), test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "colab_type": "code",
        "id": "2Ypmf9_rOU27",
        "outputId": "a2652413-1e6a-4cf5-8162-10e3c967a2a5"
      },
      "outputs": [],
      "source": [
        "dummy_prior1 = DummyClassifier(strategy='stratified')\n",
        "dummy_prior1.fit(train_features_onehot, train_labels)\n",
        "print(dummy_prior1.score(test_features_onehot, test_labels))\n",
        "evaluation_summary(\"Dummy Prior\", dummy_prior1.predict(test_features_onehot), test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6y5MB-3E8ldu"
      },
      "outputs": [],
      "source": [
        "## define pipeline with one hot encoder\n",
        "\n",
        "# Use FeatureUnion to combine the features from text and summary\n",
        "one_hot_combined_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('one-hot', CountVectorizer(tokenizer=tokenize_normalize, binary=True)), \n",
        "              ])),\n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('one-hot', CountVectorizer(tokenizer=tokenize_normalize, binary=True)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('one-hot', CountVectorizer(tokenizer=tokenize_normalize, binary=True)), \n",
        "              ])),\n",
        "        ])\n",
        "        )\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Z96eQwNzipa7"
      },
      "outputs": [],
      "source": [
        "one_hot_train_features = one_hot_combined_pipeline.fit_transform(train_threads_frame)\n",
        "one_hot_test_features = one_hot_combined_pipeline.transform(test_threads_frame)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1074
        },
        "colab_type": "code",
        "id": "OAws7GieLhaS",
        "outputId": "fa777e2e-754d-46f6-d888-451a606a5857"
      },
      "outputs": [],
      "source": [
        "## one-hot and logistic regression\n",
        "lr = LogisticRegression(solver='saga')\n",
        "lr_model = lr.fit(one_hot_train_features,train_labels)\n",
        "lr_predict = lr.predict(one_hot_test_features)\n",
        "evaluation_summary(\"LR One-Hot Encoding\", lr_predict, test_labels)                   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Igp43g0qipeS"
      },
      "outputs": [],
      "source": [
        "## one-hot and SVR\n",
        "\n",
        "svc = SVC(gamma='auto',kernel='rbf')\n",
        "svc_model = svc.fit(one_hot_train_features,train_labels)\n",
        "svc_predict = svc.predict(one_hot_test_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "colab_type": "code",
        "id": "mW4Z-P31s5bS",
        "outputId": "8cc0d5d4-c173-49b0-92d0-5648c367d939"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"SVC One-Hot Encoding\", svc_predict, test_labels)                 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2111
        },
        "colab_type": "code",
        "id": "02OXNfo9H8Kc",
        "outputId": "160df4fc-f8ce-43a7-9c28-89bbd5e7e3b5"
      },
      "outputs": [],
      "source": [
        "## one-hot and Naive Bayes\n",
        "\n",
        "nb = BernoulliNB()\n",
        "nb_model = nb.fit(one_hot_train_features, train_labels)\n",
        "nb_predict = nb.predict(one_hot_test_features)\n",
        "evaluation_summary(\"NB One-Hot Encoding\", nb_predict, test_labels)           \n",
        "print(nb_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "colab_type": "code",
        "id": "STROCBy4OhMA",
        "outputId": "7d49e45a-9679-4521-fdb3-83d854c7d76e"
      },
      "outputs": [],
      "source": [
        "#dummy\n",
        "\n",
        "##Solution\n",
        "\n",
        "dummy_prior = DummyClassifier(strategy='stratified')\n",
        "dummy_prior.fit(one_hot_train_features, train_labels)\n",
        "print(dummy_prior.score(one_hot_test_features, test_labels))\n",
        "evaluation_summary(\"Dummy Prior\", dummy_prior.predict(one_hot_test_features), test_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1023
        },
        "colab_type": "code",
        "id": "ab8uWrknO-4v",
        "outputId": "a2dbc630-ebd3-4b10-fe57-33cdc2a1a66a"
      },
      "outputs": [],
      "source": [
        "dummy_mf = DummyClassifier(strategy='most_frequent')\n",
        "dummy_mf.fit(one_hot_train_features, train_labels)\n",
        "print(dummy_mf.score(one_hot_test_features, test_labels))\n",
        "evaluation_summary(\"Dummy Majority\", dummy_mf.predict(one_hot_test_features), test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V-yMyHRZjqyq"
      },
      "source": [
        "## TFIDF \n",
        "\n",
        "All classifiers using TFIDF vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZUeAvPdMPn2a"
      },
      "source": [
        "### Hstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hLorOKMpPmj8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from scipy import sparse\n",
        "\n",
        "\n",
        "tfidf_vectorizer =TfidfVectorizer(tokenizer=tokenize_normalize, binary=False)\n",
        "train_features_tfidf_body =tfidf_vectorizer.fit_transform(train_threads_frame.body)\n",
        "test_features_tfidf_body = tfidf_vectorizer.transform(test_threads_frame.body)\n",
        "\n",
        "\n",
        "train_features_tfidf_auth =tfidf_vectorizer.fit_transform(train_threads_frame.author)\n",
        "test_features_tfidf_auth = tfidf_vectorizer.transform(test_threads_frame.author)\n",
        "\n",
        "\n",
        "train_features_tfidf_title =tfidf_vectorizer.fit_transform(train_threads_frame.title)\n",
        "test_features_tfidf_title = tfidf_vectorizer.transform(test_threads_frame.title)\n",
        "\n",
        "train_features_tfidf =sparse.hstack([train_features_tfidf_body,train_features_tfidf_auth,train_features_tfidf_title])\n",
        "\n",
        "test_features_tfidf = sparse.hstack([test_features_tfidf_body,test_features_tfidf_auth,test_features_tfidf_title])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wvv62y4lwts4"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1074
        },
        "colab_type": "code",
        "id": "EHyNekX-A4fj",
        "outputId": "2ee33fb0-8e72-455c-881f-471ab29c34c8"
      },
      "outputs": [],
      "source": [
        "## one-hot and logistic regression\n",
        "lr2 = LogisticRegression(solver='saga')\n",
        "lr_model2 = lr2.fit(train_features_tfidf,train_labels)\n",
        "lr_predict2 = lr2.predict(test_features_tfidf)\n",
        "evaluation_summary(\"LR TFIDF\", lr_predict2, test_labels)                   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mZzaqkyKwxId"
      },
      "source": [
        "### SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "colab_type": "code",
        "id": "bS4zB-kiA28X",
        "outputId": "12017532-9c0d-4acd-c623-c91914667ec9"
      },
      "outputs": [],
      "source": [
        "svc2 = SVC(gamma='auto',kernel='rbf')\n",
        "svc_model2 = svc2.fit(train_features_tfidf,train_labels)\n",
        "svc_predict2= svc2.predict(test_features_tfidf)\n",
        "evaluation_summary(\"SVC TFIDF\", svc_predict2, test_labels) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYqDvoquw0cq"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "colab_type": "code",
        "id": "n26bFrusA1AH",
        "outputId": "f0565115-4aaa-4ead-c8fb-9c4a1e84583f"
      },
      "outputs": [],
      "source": [
        "nb2 = BernoulliNB()\n",
        "nb_model2 = nb2.fit(train_features_tfidf, train_labels)\n",
        "nb_predict2 = nb2.predict(test_features_tfidf)\n",
        "evaluation_summary(\"NB TFIDF\", nb_predict2, test_labels) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V_KgUUCRw23L"
      },
      "source": [
        "### Dummy Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1023
        },
        "colab_type": "code",
        "id": "fnQfSM6gA6ny",
        "outputId": "05149588-ffa7-4201-922d-72ba68a9463e"
      },
      "outputs": [],
      "source": [
        "dummy_mf2 = DummyClassifier(strategy='most_frequent')\n",
        "dummy_mf2.fit(train_features_tfidf, train_labels)\n",
        "print(dummy_mf2.score(test_features_tfidf, test_labels))\n",
        "evaluation_summary(\"Dummy Majority TFIDF\", dummy_mf2.predict(test_features_tfidf), test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "colab_type": "code",
        "id": "1PnpbIyKAxNf",
        "outputId": "4dddde3e-e625-4272-a457-2a2f0f0ee2f5"
      },
      "outputs": [],
      "source": [
        "dummy_prior2 = DummyClassifier(strategy='stratified')\n",
        "dummy_prior2.fit(train_features_tfidf, train_labels)\n",
        "print(dummy_prior2.score(test_features_tfidf, test_labels))\n",
        "evaluation_summary(\"Dummy Prior TFIDF\", dummy_prior2.predict(test_features_tfidf), test_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dvt3ZZQAm2li"
      },
      "source": [
        "#Q2\n",
        "\n",
        "Improving effectiveness. \n",
        "In this task your goal is to improve the effectiveness of a **baseline LogisticRegression with TF-IDF vectorization**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07hN6__-nQa3"
      },
      "source": [
        "## Parameter tuning\n",
        "Required parameters: \n",
        " -   C of logistic regression (typical values might be powers of 10 (from 10^-3 to 10^4)\n",
        " -   multi_class\n",
        " -   solver\n",
        "  \n",
        "Vectorizer parameters:\n",
        " -   sublinear_tf\n",
        " -   ngram_range (from one word up to length 3)\n",
        " -   max_feature (vocabulary size) for the three fields (None to approximately 50k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1tQUsrH25ydR"
      },
      "source": [
        "### Parameters\n",
        "An array of parameters for grid search, five different combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qZFUHD5AQSIS"
      },
      "outputs": [],
      "source": [
        "\n",
        "params = [\n",
        "   {'classifier__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000)},\n",
        "   #{'classifier__solver': ('newton-cg', 'lbfgs', 'sag', 'liblinear', 'saga')},\n",
        "   \n",
        "    {'union__body__tfidf__max_features': (None, 5000, 10000, 50000),\n",
        "   'union__body__tfidf__ngram_range': ((1, 1),(1, 2), (1, 3)),\n",
        "   'union__body__tfidf__sublinear_tf': (True, False)},\n",
        "   \n",
        "    {'union__title__tfidf__max_features': (None, 5000, 10000, 50000),\n",
        "   'union__title__tfidf__ngram_range': ((1, 1),(1, 2), (1, 3)),\n",
        "   'union__title__tfidf__sublinear_tf': (True, False)},\n",
        "    {\n",
        "  'classifier__multi_class': ('ovr', 'multinomial','auto'),\n",
        "  'classifier__solver': ('newton-cg', 'lbfgs', 'sag', 'saga')},\n",
        "   \n",
        "    {'union__author__tfidf__max_features': (None, 5000, 10000, 50000),\n",
        "   'union__author__tfidf__ngram_range': ((1, 1),(1, 2), (1, 3)),\n",
        "   'union__author__tfidf__sublinear_tf': (True, False)}\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AgKwpLgx522H"
      },
      "source": [
        "### GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dbHjmb1tnboO"
      },
      "outputs": [],
      "source": [
        "\n",
        "gridsearch_results = []\n",
        "gridsearch_score = []\n",
        "  \n",
        "for param in params:\n",
        "  grid_search = GridSearchCV(tfidf_pipeline_lr, param_grid=param, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)\n",
        "  print(\"Performing grid search...\")\n",
        "  print(\"pipeline:\", [name for name, _ in tfidf_pipeline_lr.steps])\n",
        "  print(\"parameters:\")\n",
        "  print(param)\n",
        "  grid_search.fit(train_threads_frame, train_labels)\n",
        "  gridsearch_score.append(grid_search.best_score_)\n",
        "  print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "  print(\"Best parameters set:\")\n",
        "  best_parameters = grid_search.best_estimator_.get_params()\n",
        "  for param_name in sorted(param.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
        "  gridsearch_results.append(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "twL-v9C6UPd5"
      },
      "outputs": [],
      "source": [
        "gridsearch_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YBEgc1Yt56gd"
      },
      "source": [
        "### RandomizedSearch\n",
        "Attempted to perform this but stuck with grid search results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1DW4tNk0FGjB"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_iter_search=10\n",
        "random_search = RandomizedSearchCV(tfidf_pipeline_lr, param_distributions=params, n_iter=n_iter_search,n_jobs=1, verbose=1, scoring='f1_macro', cv=2)\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in tfidf_pipeline_lr.steps])\n",
        "print(\"parameters:\")\n",
        "print(params)\n",
        "\n",
        "\n",
        "random_search.fit(train_threads_frame, train_labels)\n",
        "\n",
        "print(\"Best score: %0.3f\" % random_search.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = random_search.best_estimator_.get_params()\n",
        "for param_name in sorted(params.keys()):\n",
        "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TREOw73c6Ciu"
      },
      "source": [
        "### Re-fit and execute after tuning\n",
        "Results from grid search:\n",
        "* body__TF-IDF__ngram_range\t(1,1)\n",
        "* body__TF-IDF__sublinear_tf\tTRUE\n",
        "* body__TF-IDF__max_features\t5000\n",
        "* title__TF-IDF__ngram_range\t(1,1)\n",
        "* title__TF-IDF__sublinear_tf\tTRUE\n",
        "* title__TF-IDF__max_features\t10000\n",
        "* author__TF-IDF__ngram_range\t(1,1)\n",
        "* author__TF-IDF__sublinear_tf\tTRUE\n",
        "* author__TF-IDF__max_features\t10000\n",
        "* LogisticRegression C\t10000\n",
        "* LogisticRegression solver\tsaga\n",
        "* LogisticRegression multi_class\tmultinomial\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BMEaIzkRbJ7L"
      },
      "outputs": [],
      "source": [
        " \n",
        "\n",
        "from sklearn.model_selection import ParameterGrid\n",
        " \n",
        "\n",
        "q2_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=10000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=10000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        " \n",
        "        ])\n",
        "        ),\n",
        "       ('classifier', LogisticRegression(C=10000, solver = 'saga', multi_class = 'multinomial'))\n",
        "    ])\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "tXr-SOZM8xjX",
        "outputId": "1387f0de-4ae1-40b1-98ee-192e9e410090"
      },
      "outputs": [],
      "source": [
        "q2_pipeline_model = q2_pipeline.fit(train_threads_frame, train_labels)\n",
        "q2_prediction = q2_pipeline.predict(test_threads_frame)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "colab_type": "code",
        "id": "qCOyQL2aYogB",
        "outputId": "a14b49d4-9248-48f1-d240-f68b2e60e402"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR TFIDF after Tuning\", q2_prediction, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "colab_type": "code",
        "id": "TRG-UhuDVtw2",
        "outputId": "070d75da-a2cb-4134-8044-95cf6652908a"
      },
      "outputs": [],
      "source": [
        "precision, reacall, fscore, support = score(q2_prediction,test_labels)\n",
        "\n",
        "bars = np.arange(len(q2_pipeline_model.classes_))\n",
        "classes = q2_pipeline_model.classes_\n",
        "width=.4\n",
        "fig, ax=plt.subplots(1,1,figsize=(8,6))\n",
        "rects1 = ax.bar(bars, fscore, width)\n",
        "\n",
        "ax.set_ylabel('F1')\n",
        "ax.set_title('Logistic Regression Improved Model F1 scores')\n",
        "ax.set_xticks(bars+width/3)\n",
        "plt.xticks(rotation='vertical')\n",
        "ax.set_xticklabels(classes)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# fig.savefig('fscores_barplot.png', format='png', dpi=320, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "colab_type": "code",
        "id": "YCaHGcKRFf7H",
        "outputId": "e3b1a421-dcdf-46c9-a7cf-24341137bb38"
      },
      "outputs": [],
      "source": [
        "plot_conf_matrix(test_labels,q2_prediction, q2_pipeline_model.classes_, title= 'Confusion Matrix')\n",
        "plot_confusion_matrix(test_labels, q2_prediction, classes=q2_pipeline_model.classes_,title='Confusion matrix, without normalization')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a4pZcDL4ejie"
      },
      "source": [
        "## Error Analysis\n",
        "* Manually examine the examples that your algorithm made prediction errors on. \n",
        "* See if you spot any systematic trend in what type of examples it is making errors on. \n",
        "* You will report on and summarize these findings\n",
        "\n",
        "Basically found that if a reference to any of the other "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1666
        },
        "colab_type": "code",
        "id": "yc3T6r9r80mu",
        "outputId": "c64c7fd7-0b94-44b8-b025-1a414a8a8aca"
      },
      "outputs": [],
      "source": [
        "print(train_threads[train_threads.subreddit=='starcraft'].iloc[0].title)\n",
        "train_threads[train_threads.subreddit=='starcraft'].iloc[0].posts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fkkexERpELy6"
      },
      "source": [
        "## Feature Analysis\n",
        "Inspired by the error analysis, engineer two new features you develop to try to improve\n",
        "the effectiveness. Ideas include:\n",
        "* Length of posts, lengths of threads, etc\n",
        "* TF-IDF values computed from the full Reddit dataset (or larger external data)\n",
        "* Word embedding features\n",
        "* Tokenizer modifications (stop word handling, etc)\n",
        "\n",
        "Add your feature to the model and retrain. The new features does not have to improve effectiveness\n",
        "\n",
        "For both (i) and (ii), include in your report \n",
        "1. changes to the model and how they were developed. \n",
        "2. report the results (same as Q1) on the test data. The final model is expected to outperform the LR TF-IDF baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dq2EWi8Jg1pV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_features_frame(subreddit_file):\n",
        "  \n",
        "  posts = list()\n",
        "\n",
        "  # If the dataset is too large, you can load a subset of the posts.\n",
        "  post_limit = 100000000\n",
        "  # Construct a dataframe, by opening the JSON file line-by-line\n",
        "  with open(subreddit_file) as jsonfile:\n",
        "    for i, line in enumerate(jsonfile):\n",
        "      thread = json.loads(line)\n",
        "      \n",
        "      if (len(posts) > post_limit):\n",
        "        break\n",
        "      #first_post = True\n",
        "      \n",
        "      #lent = len(thread['posts'])\n",
        "      count = 0\n",
        "      avg_body_length=0.0\n",
        "      total_comments = 0\n",
        "      sub=''\n",
        "      title=''\n",
        "      url=''\n",
        "      id_=''\n",
        "      author=''\n",
        "      body=''\n",
        "      body_length=0\n",
        "      post_depth=0\n",
        "      majority_type=''\n",
        "      \n",
        "      for post in thread['posts']:\n",
        "        \n",
        "        \n",
        "        total_comments = len(thread['posts'])     \n",
        "        sub =thread['subreddit']\n",
        "        title = thread['title']\n",
        "        url = thread['url']\n",
        "        id_ += \" | \" +post.get('id',\"\")\n",
        "        author+= \" | \" + post.get('author', \"\")\n",
        "        body+=\" | \" +post.get('body', \"\")\n",
        "        avg_length = avg_body_length        \n",
        "        body_length = len(body)\n",
        "        majority_type += \" | \" + post.get('majority_type',\"\")\n",
        "      \n",
        "      avg_body_length = body_length/total_comments      \n",
        "\n",
        "      posts.append((sub, title, url,id_, author, body,\n",
        "                         body_length,majority_type\n",
        "                          ,total_comments,(body_length/total_comments) ) )\n",
        "  labels = ['subreddit', 'title', 'url', 'id', 'author', 'body', 'body_length','majority_type', 'post_length', 'avg_length']\n",
        "  return(pd.DataFrame(posts, columns=labels))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eQtL-X1LhIPA"
      },
      "outputs": [],
      "source": [
        "train_threads_frame2 = create_features_frame(subreddit_train)\n",
        "test_threads_frame2 = create_features_frame(subreddit_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "colab_type": "code",
        "id": "bdLzioCYiIPg",
        "outputId": "0cca2fa7-c00a-4b9c-acf0-df76cdb48546"
      },
      "outputs": [],
      "source": [
        "train_threads_frame2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fq7FkDgfmo00"
      },
      "source": [
        "### Re-fit and predict model\n",
        "After analysis, decided to add the following features:\n",
        "- majority type\n",
        "- body length\n",
        "- post length\n",
        "- average length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Fd3D19nRo_d"
      },
      "source": [
        "#### Pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Kg6DKMQCMpa2"
      },
      "outputs": [],
      "source": [
        "## Wrapper to return the dataframe column incase of non-string values \n",
        "def get_body_length(df):\n",
        "  return df['body_length'].values.reshape(len(df),1)\n",
        "call_body_length = FunctionTransformer(get_body_length, validate=False)\n",
        "\n",
        "def get_post_length(df):\n",
        "  return df['post_length'].values.reshape(len(df),1)\n",
        "call_post_length = FunctionTransformer(get_post_length, validate=False)\n",
        "\n",
        "def get_avg_length(df):\n",
        "  return df['body_length'].values.reshape(len(df),1)\n",
        "call_avg_length = FunctionTransformer(get_avg_length, validate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IcdnPkqJOoGF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "q2_majority_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "           \n",
        "            ('majority_type', Pipeline([\n",
        "              ('selector', ItemSelector(key='majority_type')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "        \n",
        "        ])\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "colab_type": "code",
        "id": "10CuyhKYPBk_",
        "outputId": "ba7e667e-8ad4-412b-a6f6-ac8953fd17e4"
      },
      "outputs": [],
      "source": [
        "q2_train_features_1 = q2_majority_pipeline.fit_transform(train_threads_frame2)\n",
        "q2_test_features_1 = q2_majority_pipeline.transform(test_threads_frame2)\n",
        "\n",
        "\n",
        "q2_lr_1 = LogisticRegression(C=10000, solver = 'newton-cg', multi_class = 'multinomial')\n",
        "q2_lr_model_1 = q2_lr_1.fit(q2_train_features_1,train_labels)\n",
        "q2_lr_predict_1 = q2_lr_1.predict(q2_test_features_1)\n",
        "\n",
        "evaluation_summary(\"LR Majority Type Feature\", q2_lr_predict_1, test_labels)                                \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Z_z7LWW9OoTF"
      },
      "outputs": [],
      "source": [
        "q2_length_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "            ('body_length', call_body_length),\n",
        "            ('avg_length', call_avg_length),\n",
        "            ('post_length', call_post_length),             \n",
        "        ])\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aCmxBz2-O_y0"
      },
      "outputs": [],
      "source": [
        "q2_train_features_2 = q2_length_pipeline.fit_transform(train_threads_frame2)\n",
        "q2_test_features_2 = q2_length_pipeline.transform(test_threads_frame2)\n",
        "\n",
        "\n",
        "q2_lr_2 = LogisticRegression(C=10000, solver = 'newton-cg', multi_class = 'multinomial')\n",
        "q2_lr_model_2 = q2_lr_2.fit(q2_train_features_2,train_labels)\n",
        "q2_lr_predict_2 = q2_lr_2.predict(q2_test_features_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "colab_type": "code",
        "id": "FZVBqxe5RTE4",
        "outputId": "7ae8ad57-4279-4cc4-c992-2a40d95a75ee"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR Lengths Features\", q2_lr_predict_2, test_labels)                                \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4eWowUjhlsMh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "q2_final_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "           \n",
        "            ('majority_type', Pipeline([\n",
        "              ('selector', ItemSelector(key='majority_type')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "\n",
        "            ('body_length', call_body_length),\n",
        "            ('avg_length', call_avg_length),\n",
        "            ('post_length', call_post_length),             \n",
        "        ])\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "-ycc1wPDltyb",
        "outputId": "2e566ec5-1929-4a65-ba7b-a40f1c7a6096"
      },
      "outputs": [],
      "source": [
        "q2_train_features = q2_final_pipeline.fit_transform(train_threads_frame2)\n",
        "q2_test_features = q2_final_pipeline.transform(test_threads_frame2)\n",
        "\n",
        "\n",
        "q2_lr = LogisticRegression(C=10000, solver = 'newton-cg', multi_class = 'multinomial')\n",
        "q2_lr_model = q2_lr.fit(q2_train_features,train_labels)\n",
        "q2_lr_predict = q2_lr.predict(q2_test_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "colab_type": "code",
        "id": "9Xjt90nYluHt",
        "outputId": "3808d473-2bf5-40a3-fdc3-d28ed380455b"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR Combined Features\", q2_lr_predict, test_labels)                                \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0klw3xODbFni"
      },
      "source": [
        "#### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "colab_type": "code",
        "id": "HSaGyERNlxLb",
        "outputId": "28fd2539-00c6-44fa-8993-84d550126a44"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_confusion_matrix(test_labels, q2_lr_predict, q2_lr_model.classes_,title='Confusion matrix, without normalization')\n",
        "plot_conf_matrix(test_labels,q2_lr_predict, q2_lr_model.classes_, title= 'Confusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "colab_type": "code",
        "id": "R4_OkYeXKyVL",
        "outputId": "7aead6c6-3b61-4b0a-891f-5057815b13bb"
      },
      "outputs": [],
      "source": [
        "create_plot('Logistic Regression Model F1', q2_lr_model, q2_lr_predict, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D-G938e8D5qn"
      },
      "source": [
        "# Part B: Discourse prediction ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "InmBeB-9bLsT"
      },
      "source": [
        "## Q3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "9KM6aJtSETPY",
        "outputId": "2a97d703-2a01-4021-e954-2f7a13d43ba8"
      },
      "outputs": [],
      "source": [
        "discourse_train = \"coursework_discourse_train.json\"\n",
        "discourse_test = \"coursework_discourse_test.json\"\n",
        "  \n",
        "!gsutil cp gs://textasdata/coursework/coursework_discourse_train.json $discourse_train  \n",
        "!gsutil cp gs://textasdata/coursework/coursework_discourse_test.json  $discourse_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pwOaf_6aD-Vh"
      },
      "outputs": [],
      "source": [
        "# The reddit thread structure is nested with posts in a new content.\n",
        "# This block reads the file as json and creates a new data frame.\n",
        "\n",
        "def load_posts(file):\n",
        "  # A temporary variable to store the list of post content.\n",
        "  posts_tmp = list()\n",
        "\n",
        "  with open(file) as jsonfile:\n",
        "    for i, line in enumerate(jsonfile):\n",
        "     # if (i > 2): break\n",
        "      thread = json.loads(line)\n",
        "      for post in thread['posts']:\n",
        "        # NOTE: This could be changed to use additional features from the post or thread.\n",
        "        # DO NOT change the labels for the test set.\n",
        "        posts_tmp.append((thread['subreddit'], thread['title'], thread['url'],\n",
        "                        post['id'], post.get('author', \"\"), post.get('body', \"\"), post.get(\"majority_link\", \"\"), \n",
        "                        post.get('post_depth', 0), post.get('majority_type', \"\"), # discourse type label \n",
        "                        post.get('in_reply_to', \"\") ))\n",
        "\n",
        "# Create the posts data frame.  \n",
        "  labels = ['subreddit', 'title', 'url', 'id', 'author', 'body', 'majority_link', \n",
        "          'post_depth', 'discourse_type', 'in_reply_to']\n",
        "  return pd.DataFrame(posts_tmp, columns=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "PDzHTDcmEQ11",
        "outputId": "f0e9fa32-63d9-4416-c5a6-f418f1dd44b8"
      },
      "outputs": [],
      "source": [
        "train_posts = load_posts(discourse_train)\n",
        "# Filter out empty labels\n",
        "train_posts = train_posts[train_posts['discourse_type'] != \"\"]\n",
        "print(\"Num posts: \", train_posts.size)\n",
        "train_posts.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dqGLzyTOGadY"
      },
      "source": [
        "The label for the post we will be predicting is in the discourse_type column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "7vvo7hfCEmvj",
        "outputId": "b7a76e1e-8d30-4377-8bcf-cb961b7c7186"
      },
      "outputs": [],
      "source": [
        "test_posts = load_posts(discourse_test)\n",
        "# Filter out empty labels\n",
        "test_posts = test_posts[test_posts['discourse_type'] != \"\"]\n",
        "print(\"Num posts: \", test_posts.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Jat55HhNHGBp"
      },
      "outputs": [],
      "source": [
        "train_labels_p2 = train_posts['discourse_type']\n",
        "test_labels_p2 = test_posts['discourse_type']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rFl6sM58HNFp"
      },
      "source": [
        "Examine the distribution over labels on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "colab_type": "code",
        "id": "n3NbLPBhFOkp",
        "outputId": "298fb26a-f984-4f1a-d468-7b7e4ecdd797"
      },
      "outputs": [],
      "source": [
        "discourse_counts = train_labels_p2.value_counts()\n",
        "print(discourse_counts.describe())\n",
        "\n",
        "top_discourse = discourse_counts.nlargest(200)\n",
        "print(top_discourse)\n",
        "top_discourse = top_discourse.index.tolist()\n",
        "print(top_discourse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "acupjNXed_IW"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "VMb3_qcpXwV_",
        "outputId": "22b391e4-8441-466a-94e0-eef71ed8b299"
      },
      "outputs": [],
      "source": [
        "disc_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "             \n",
        "        ])\n",
        "        ),\n",
        "       ('classifier', LogisticRegression(C=10000, solver = 'saga', multi_class = 'multinomial'))\n",
        "    ])\n",
        "\n",
        "disc_pipe_model = disc_pipeline.fit(train_posts, train_labels_p2)\n",
        "prediction_disc = disc_pipeline.predict(test_posts)\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "colab_type": "code",
        "id": "faUuB-OCaBAe",
        "outputId": "47081af0-a505-4b6d-841c-4153ce596280"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR TFIDF Discourse\", prediction_disc, test_labels_p2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u0o6NSkVedFa"
      },
      "source": [
        "#### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "colab_type": "code",
        "id": "wwAFvDO2bVrb",
        "outputId": "67efa1a4-99c5-4819-9bdf-8db4ce1f7dbd"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_confusion_matrix(test_labels_p2, prediction_disc, classes=disc_pipe_model.classes_,title='Confusion matrix, without normalization')\n",
        "\n",
        "plot_conf_matrix(test_labels_p2,prediction_disc, disc_pipe_model.classes_, title= 'Confusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "colab_type": "code",
        "id": "gKmRasmpb77K",
        "outputId": "63f693f3-b45c-49d8-90b9-ce8e5e8765bf"
      },
      "outputs": [],
      "source": [
        "create_plot('Logistic Regression Model F1', disc_pipe_model, prediction_disc, test_labels_p2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-_tLsom7eS5-"
      },
      "source": [
        "### GridSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7GXmSk-FeTHA"
      },
      "source": [
        "### Re-fit and predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_iF71vXn0kx"
      },
      "source": [
        "## Q4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yN5fkwTCnyqP"
      },
      "outputs": [],
      "source": [
        "# The reddit thread structure is nested with posts in a new content.\n",
        "# This block reads the file as json and creates a new data frame.\n",
        "\n",
        "\n",
        "def load_features(file):\n",
        "  # A temporary variable to store the list of post content.\n",
        "  posts_tmp = list()\n",
        "\n",
        "  with open(file) as jsonfile:\n",
        "    for i, line in enumerate(jsonfile):\n",
        "     # if (i > 2): break\n",
        "      thread = json.loads(line)\n",
        "      \n",
        "      count = 0\n",
        "      avg_body_length=0.0\n",
        "      post_length = 0\n",
        "      body_length=0\n",
        "      qpunc = ''\n",
        "      total_comments=0\n",
        "      \n",
        "      for post in thread['posts']:\n",
        "          \n",
        "        body = post.get('body', \"\")\n",
        "        post_length += len(body)\n",
        "        total_comments +=1\n",
        "        count+=1\n",
        "\n",
        "      #avg_body_length = total_comments/count\n",
        "      firstPost = True\n",
        "      \n",
        "      for post in thread['posts']:\n",
        "        body = post.get('body', \"\")\n",
        "        body_length = len(body)\n",
        "\n",
        "        post_depth =''\n",
        "          \n",
        "        ## Post depth bins\n",
        "        if post.get('post_depth',\"\") == 1:\n",
        "          post_depth='group A'\n",
        "        elif post.get('post_depth',\"\")==2:\n",
        "          post_depth='group B'\n",
        "        elif post.get('post_depth',\"\")==3:\n",
        "          post_depth='group B'\n",
        "        elif post.get('post_depth',\"\")==4:\n",
        "          post_depth='group C'\n",
        "        else:\n",
        "          post_depth='group C'\n",
        "\n",
        "                \n",
        "    \n",
        "        if post.get('is_self_post',\"\")==1.0:\n",
        "          self_post= 'Yes'\n",
        "        else:\n",
        "          self_post = 'No'\n",
        "             \n",
        "        firstPost=False\n",
        "        ## lengths\n",
        "        length_group=''\n",
        "        if(body_length < 10):\n",
        "          length_group = 'Group A'\n",
        "        elif(body_length > 10 & body_length< 50):\n",
        "          length_group = 'Group B'\n",
        "        elif(body_length > 50 & body_length< 200):\n",
        "          length_group = 'Group C'\n",
        "        elif(body_length > 200 & body_length< 500):\n",
        "          length_group = 'Group D'\n",
        "        elif(body_length > 500 & body_length< 1000):\n",
        "          length_group = 'Group E'\n",
        "        elif(body_length > 1000 & body_length< 2000):\n",
        "          length_group = 'Group F'\n",
        "        elif(body_length > 2000 & body_length< 3000):\n",
        "          length_group = 'Group G'\n",
        "        else:\n",
        "          length_group = 'Group H'\n",
        "        \n",
        "        #self author\n",
        "        same_auth=0\n",
        "        if( thread['is_self_post']==None):\n",
        "          thread['is_self_post']=0\n",
        "        thread['top_author']=thread['posts'][0].get('author',\"\")\n",
        "        \n",
        "        if(thread['top_author']==post.get('author',\"\")):\n",
        "          same_auth=1\n",
        "        \n",
        "        \n",
        "        ##comments\n",
        "        total_comments = len(thread['posts'])\n",
        "        sub_auth = thread['subreddit'] + \",\" +  post.get('author', \"\")\n",
        "        posts_tmp.append((self_post,thread['subreddit'], thread['title'], thread['url'],\n",
        "                        post['id'], post.get('author', \"\"),body, post.get(\"majority_link\", \"\"), \n",
        "                        post.get('post_depth', 0), post.get('majority_type', \"\"),  \n",
        "                        post.get('in_reply_to', \"\"), post_depth, total_comments, length_group, sub_auth, same_auth))\n",
        "\n",
        "# Create the posts data frame.  \n",
        "  labels = ['selfpost','subreddit', 'title', 'url', 'id', 'author', 'body', 'majority_link', \n",
        "          'post_depth', 'discourse_type', 'in_reply_to', 'post_depthcat', 'total_comments','body_length','sub_auth','same_auth']\n",
        "  return pd.DataFrame(posts_tmp, columns=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "sUNOxe2DtAFp",
        "outputId": "4b93f82c-4b8d-48f4-c379-f934f9fe2001"
      },
      "outputs": [],
      "source": [
        "train_posts_q4 = load_features(discourse_train)\n",
        "# Filter out empty labels\n",
        "train_posts_q4 = train_posts_q4[train_posts_q4['discourse_type'] != \"\"]\n",
        "print(\"Num posts: \", train_posts_q4.shape)\n",
        "\n",
        "test_posts_q4 = load_features(discourse_test)\n",
        "# Filter out empty labels\n",
        "test_posts_q4 = test_posts_q4[test_posts_q4['discourse_type'] != \"\"]\n",
        "print(\"Num posts: \", test_posts_q4.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "colab_type": "code",
        "id": "pbVa1fkZGHVQ",
        "outputId": "1a0dc275-0121-4b13-eb38-9a2d55bbbb56"
      },
      "outputs": [],
      "source": [
        "train_posts.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "colab_type": "code",
        "id": "rsSCstNOQT6V",
        "outputId": "9fc73b67-2829-47ca-a864-448ec299c619"
      },
      "outputs": [],
      "source": [
        "test_posts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "P9MEQ0qoipWN"
      },
      "outputs": [],
      "source": [
        "# print(train_threads[train_threads.subreddit=='starcraft'].iloc[:10].title)\n",
        "# train_threads[train_threads.subreddit=='starcraft'].iloc[1].posts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GVauoiJsh1UV"
      },
      "source": [
        "## Adding features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4MuF3U4FCOew"
      },
      "outputs": [],
      "source": [
        "# This is the tokenizer that removes punctuation\n",
        "def normalize2(tokens):\n",
        "  normalized_tokens = list()\n",
        "  for token in tokens:\n",
        "    normalized = token.text.lower().strip()\n",
        "    if ((token.is_alpha or token.is_digit or token.is_punct )):\n",
        "      normalized_tokens.append(normalized)\n",
        "  return normalized_tokens\n",
        "\n",
        "#@Tokenize and normalize\n",
        "def punctuation_tokenize_normalize(string):\n",
        "  return normalize2(spacy_tokenize(string))\n",
        "\n",
        "## Wrapper to return the dataframe column incase of non-string values \n",
        "\n",
        "# Thread features\n",
        "def get_comments(df):\n",
        "  return df['total_comments'].values.reshape(len(df),1)\n",
        "call_total_comments = FunctionTransformer(get_comments, validate=False)\n",
        "\n",
        "# Author feature\n",
        "def get_self_author(df):\n",
        "  return df['same_auth'].values.reshape(len(df),1)\n",
        "call_self_author = FunctionTransformer(get_self_author, validate=False)\n",
        "\n",
        "# Structure feature\n",
        "def get_body_length(df):\n",
        "  return df['body_length'].values.reshape(len(df),1)\n",
        "call_body_length = FunctionTransformer(get_body_length, validate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "isK9h2r4GtYn",
        "outputId": "aeded327-0f22-403d-d701-ca78f08997d7"
      },
      "outputs": [],
      "source": [
        "# explanation of tokenizer\n",
        "string = 'No! this game, so ridiculous!'\n",
        "# tokenizer that removes punctuation\n",
        "print(tokenize_normalize(string))\n",
        "\n",
        "#tokenizer that keeps punctuation\n",
        "print(punctuation_tokenize_normalize(string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "seitHecDiix3"
      },
      "source": [
        "### Community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "EcEZ6leph600",
        "outputId": "cc57b306-d5e1-4ed1-97b2-8dff65fd3a50"
      },
      "outputs": [],
      "source": [
        "community_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "           \n",
        "            #Community\n",
        "            ('subreddit', Pipeline([\n",
        "              ('selector', ItemSelector(key='subreddit')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "             \n",
        "        ])\n",
        "        ),\n",
        "       ('classifier', LogisticRegression(C=10000, solver = 'saga', multi_class = 'multinomial'))\n",
        "    ])\n",
        "   \n",
        "community_pipeline.fit(train_posts_q4, train_labels_p2)\n",
        "community_prediction = community_pipeline.predict(test_posts_q4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1801
        },
        "colab_type": "code",
        "id": "aD_ZETApj1Vu",
        "outputId": "cc5659d3-d3f4-439f-d72e-ea2afac2be9f"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR Baseline with Author feature\", community_prediction, test_labels_p2)      \n",
        "\n",
        "plot_confusion_matrix(test_labels_p2, community_prediction, classes=community_pipeline.classes_,title='Confusion matrix, LR with Author feature')\n",
        "plot_conf_matrix(test_labels_p2,community_prediction, community_pipeline.classes_, title= 'Confusion Matrix')\n",
        "\n",
        "create_plot('LR Model with Author feature Chart', community_pipeline, community_prediction, test_labels_p2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zOSBOKeKimQa"
      },
      "source": [
        "### Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "TeB1qXiYiCHV",
        "outputId": "1161e437-6429-40e3-e5c9-fc2986c6b5e1"
      },
      "outputs": [],
      "source": [
        "structure_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "           \n",
        "              \n",
        "            #structure\n",
        "            ('post_depthcat', Pipeline([\n",
        "              ('selector', ItemSelector(key='post_depthcat')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "            ('body_length', Pipeline([\n",
        "              ('selector', ItemSelector(key='body_length')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "\n",
        "             \n",
        "        ])\n",
        "        ),\n",
        "       ('classifier', LogisticRegression(C=10000, solver = 'saga', multi_class = 'multinomial'))\n",
        "    ])\n",
        "\n",
        "\n",
        "   \n",
        "structure_pipeline.fit(train_posts_q4, train_labels_p2)\n",
        "structure_prediction = structure_pipeline.predict(test_posts_q4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1800
        },
        "colab_type": "code",
        "id": "utThSvoflL5S",
        "outputId": "607959a5-d353-4172-f5d7-d11dd291e3ed"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR Baseline with structure feature\", structure_prediction, test_labels_p2)      \n",
        "plot_conf_matrix(test_labels_p2,structure_prediction, structure_pipeline.classes_, title= 'Confusion Matrix')\n",
        "plot_confusion_matrix(test_labels_p2, structure_prediction, classes=structure_pipeline.classes_,title='Confusion matrix, LR with structure feature')\n",
        "create_plot('LR Model with structure feature Chart', structure_pipeline, structure_prediction, test_labels_p2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iqwiBOB3iqf_"
      },
      "source": [
        "### Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "RQGu1ZWgiCyk",
        "outputId": "518d4c83-faf8-4955-d787-e026d084e025"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "metadata_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "           \n",
        "           \n",
        "              \n",
        "            #Metadata \n",
        "            ('sub_auth', Pipeline([\n",
        "              ('selector', ItemSelector(key='sub_auth')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=punctuation_tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "\n",
        "             \n",
        "        ])\n",
        "        ),\n",
        "       ('classifier', LogisticRegression(C=10000, solver = 'saga', multi_class = 'multinomial'))\n",
        "    ])\n",
        "   \n",
        "metadata_pipe_model = metadata_pipeline.fit(train_posts_q4, train_labels_p2)\n",
        "metadata_prediction = metadata_pipe_model.predict(test_posts_q4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1800
        },
        "colab_type": "code",
        "id": "O5VvphGumtGP",
        "outputId": "8d8a0681-fba3-4842-d988-65ed6d9dde82"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR Baseline with metadata feature\", metadata_prediction, test_labels_p2)      \n",
        "plot_conf_matrix(test_labels_p2,metadata_prediction, metadata_pipe_model.classes_, title= 'Confusion Matrix')\n",
        "plot_confusion_matrix(test_labels_p2, metadata_prediction, classes=metadata_pipe_model.classes_,title='Confusion matrix, LR with metadata feature')\n",
        "create_plot('LR Model with metadata feature Chart', metadata_pipe_model, metadata_prediction, test_labels_p2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BKenZXd_ivSR"
      },
      "source": [
        "### Content + Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "1z2jNVJ6iDap",
        "outputId": "6ad71e5b-8c5a-4438-9618-a057d38560d7"
      },
      "outputs": [],
      "source": [
        "cp_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            #Content+punctuation \n",
        "            ('body_punc', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=punctuation_tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "             \n",
        "        ])\n",
        "        ),\n",
        "       ('classifier', LogisticRegression(C=10000, solver = 'saga', multi_class = 'multinomial'))\n",
        "    ])\n",
        "\n",
        "#one line wrapper that is vectorizer\n",
        "\n",
        "   \n",
        "cp_model = cp_pipeline.fit(train_posts_q4, train_labels_p2)\n",
        "cp_prediction = cp_pipeline.predict(test_posts_q4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1800
        },
        "colab_type": "code",
        "id": "PlygMyOhoPet",
        "outputId": "9cca1e2e-76b0-462c-f9ae-adf8e865f80a"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR Baseline with  content+punctuation feature\", cp_prediction, test_labels_p2)      \n",
        "plot_conf_matrix(test_labels_p2,cp_prediction, cp_model.classes_, title= 'Confusion Matrix')\n",
        "plot_confusion_matrix(test_labels_p2, cp_prediction, classes=cp_model.classes_,title='Confusion matrix, LR with  content+punctuation feature')\n",
        "create_plot('LR Model with content+punctuation feature Chart', cp_model, cp_prediction, test_labels_p2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4fb3W5EXi2WY"
      },
      "source": [
        "### Author"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "NAYD8_xbiW2J",
        "outputId": "7cda91d8-f9c0-4180-8ca6-11a46fef4776"
      },
      "outputs": [],
      "source": [
        "author_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            #same auth \n",
        "            ('self_author', call_self_author),\n",
        "              \n",
        "              \n",
        "             \n",
        "        ])\n",
        "        ),\n",
        "       ('classifier', LogisticRegression(C=10000, solver = 'saga', multi_class = 'multinomial'))\n",
        "    ])\n",
        "\n",
        "\n",
        "   \n",
        "author_model = author_pipeline.fit(train_posts_q4, train_labels_p2)\n",
        "author_prediction = author_pipeline.predict(test_posts_q4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1800
        },
        "colab_type": "code",
        "id": "TJgm1-DFou98",
        "outputId": "8b04017d-b709-42d5-beb9-84d8def12895"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR Baseline with author feature\", author_prediction, test_labels_p2)      \n",
        "plot_conf_matrix(test_labels_p2,author_prediction, author_model.classes_, title= 'Confusion Matrix')\n",
        "plot_confusion_matrix(test_labels_p2, author_prediction, classes=author_model.classes_,title='Confusion matrix, LR with author feature')\n",
        "create_plot('LR Model with author feature Chart', author_model, author_prediction, test_labels_p2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IOQ_cZZei5Yn"
      },
      "source": [
        "### Total Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "BhgS-vJAicwL",
        "outputId": "74ebf6ec-515f-40fc-ba3f-9c61f772acdc"
      },
      "outputs": [],
      "source": [
        "comments_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            #Thread features\n",
        "            ('Total_comments', call_total_comments),\n",
        "              \n",
        "              \n",
        "             \n",
        "        ])\n",
        "        ),\n",
        "       ('classifier', LogisticRegression(C=10000, solver = 'saga', multi_class = 'multinomial'))\n",
        "    ])\n",
        "\n",
        "\n",
        "   \n",
        "comments_model = comments_pipeline.fit(train_posts_q4, train_labels_p2)\n",
        "comments_prediction = comments_pipeline.predict(test_posts_q4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1922
        },
        "colab_type": "code",
        "id": "r1hWQxJ4vunS",
        "outputId": "80d14738-ae7b-4917-b503-c9bc50e355c3"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR Baseline with total comments feature\", comments_prediction, test_labels_p2)      \n",
        "plot_conf_matrix(test_labels_p2,comments_prediction, comments_model.classes_, title= 'Confusion Matrix')\n",
        "plot_confusion_matrix(test_labels_p2, comments_prediction, classes=comments_model.classes_,title='Confusion matrix, LR with total comments feature')\n",
        "create_plot('LR Model with total comments feature Chart', comments_model, comments_prediction, test_labels_p2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ryHHrGs-h7Yj"
      },
      "source": [
        "### Combined Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oo7rYCRviuEV"
      },
      "outputs": [],
      "source": [
        "combined_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              \n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False, max_features=5000, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "              \n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "           \n",
        "            #Community\n",
        "            ('subreddit', Pipeline([\n",
        "              ('selector', ItemSelector(key='subreddit')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "            #structure\n",
        "            ('post_depthcat', Pipeline([\n",
        "              ('selector', ItemSelector(key='post_depthcat')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "            ('body_length', Pipeline([\n",
        "              ('selector', ItemSelector(key='body_length')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "            #Metadata \n",
        "            ('sub_auth', Pipeline([\n",
        "              ('selector', ItemSelector(key='sub_auth')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "            #Content+punctuation \n",
        "            ('body_punc', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tfidf', TfidfVectorizer(tokenizer=punctuation_tokenize_normalize, binary=False,max_features=None, ngram_range=(1,1), sublinear_tf=True)), \n",
        "              ])),\n",
        "            #same auth \n",
        "            ('self_author', call_self_author),\n",
        "            #Thread features\n",
        "            ('Total_comments', call_total_comments),\n",
        "             \n",
        "        ])\n",
        "        )\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "h64UBWNE2b9z"
      },
      "outputs": [],
      "source": [
        "final_train_features = combined_pipeline.fit_transform(train_posts_q4)\n",
        "final_test_features = combined_pipeline.transform(test_posts_q4)\n",
        "\n",
        "\n",
        "final_lr = LogisticRegression(C=10000, solver = 'saga', multi_class = 'multinomial')\n",
        "\n",
        "lr_final_model = final_lr.fit(final_train_features,train_labels_p2)\n",
        "lr_final_prediction = final_lr.predict(final_test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1800
        },
        "colab_type": "code",
        "id": "HBUuXrcw0NFT",
        "outputId": "33e6096e-2467-4601-9007-ad7328fa56e5"
      },
      "outputs": [],
      "source": [
        "evaluation_summary(\"LR Combined Features\", lr_final_prediction, test_labels_p2) \n",
        "plot_conf_matrix(test_labels_p2,lr_final_prediction, lr_final_model.classes_, title= 'Confusion Matrix')\n",
        "plot_confusion_matrix(test_labels_p2, lr_final_prediction, classes=lr_final_model.classes_,title='Confusion matrix, LR with Combined Features')\n",
        "create_plot('LR Model with Combined Features Chart', lr_final_model, lr_final_prediction, test_labels_p2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oCzNIo3dGkPG"
      },
      "source": [
        "### Feature performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "QhQr6XAS3rXG",
        "outputId": "90a34ec6-3be0-4cc9-b688-5da2089df296"
      },
      "outputs": [],
      "source": [
        "!pip install eli5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "colab_type": "code",
        "id": "h_cARUmDmYm5",
        "outputId": "b55bb428-17b8-4e40-e3ac-f60bc21f6216"
      },
      "outputs": [],
      "source": [
        "import eli5\n",
        "eli5.show_weights(final_lr, top=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "noHrsiU5FrJQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "2416724k_TADCoursework.ipynb",
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "nlpenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
